{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = 'kIeG3SQnyatTNzP2SKF_eg'\n",
    "secret_key = 'hnXPG4wASTbzxDwY9iW01HvG8_WrIg'\n",
    "\n",
    "auth = requests.auth.HTTPBasicAuth(client_id, secret_key)\n",
    "\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': 'veochae',\n",
    "    'password': 'Hold5489!!'\n",
    "}\n",
    "\n",
    "headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token', \n",
    "                    auth = auth, \n",
    "                    data = data,\n",
    "                    headers = headers)\n",
    "\n",
    "token = res.json()['access_token']\n",
    "\n",
    "headers['Authorization'] = f'bearer {token}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_data(time_wanted):\n",
    "    stopper = 0\n",
    "\n",
    "    #initial set collection\n",
    "    res = requests.get('https://oauth.reddit.com/r/Dreams/new',\n",
    "                    headers = headers, params={'limit': '100'})\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for post in res.json()['data']['children']:\n",
    "        df = pd.concat([df,pd.DataFrame({'subreddit': post['data']['subreddit'],\n",
    "                                                    'title': post['data']['title'],\n",
    "                                                    'text': post['data']['selftext'],\n",
    "                                                    'date': post['data']['created']},index=[0])], ignore_index= True)\n",
    "    \n",
    "    #further back collection\n",
    "    latest_key = post['kind'] + '_' + post['data']['id']\n",
    "\n",
    "    while df.tail(1)['date'][df.tail(1)['date'].index[0]] > datetime.timestamp(time_wanted):\n",
    "        for req in range(100):\n",
    "        \n",
    "            res = requests.get('https://oauth.reddit.com/r/Dreams/new',\n",
    "                                headers = headers, \n",
    "                                params={'limit': '100', 'after': latest_key})\n",
    "            \n",
    "            for post in res.json()['data']['children']:\n",
    "                df = pd.concat([df,pd.DataFrame({'subreddit': post['data']['subreddit'],\n",
    "                                                    'title': post['data']['title'],\n",
    "                                                    'text': post['data']['selftext'],\n",
    "                                                    'date': post['data']['created']},index=[0])], ignore_index= True)\n",
    "\n",
    "            latest_key = post['kind'] + '_' + post['data']['id']\n",
    "                \n",
    "            print(f'{len(df)} rows collected')\n",
    "\n",
    "            if len(df) >= 988:\n",
    "                latest = df.tail(1)['date'][df.tail(1)['date'].index[0]]\n",
    "                print(\"Data Collection Target Reached\")\n",
    "                print(f'{len(df)} rows collected')\n",
    "                print(f'latest subreddit date: {datetime.fromtimestamp(latest)}')\n",
    "                return df\n",
    "\n",
    "            # time.sleep(5)\n",
    "    else: \n",
    "        print(\"Date Limit Reached\")\n",
    "        print(f'{len(df)} rows collected')\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 rows collected\n",
      "300 rows collected\n",
      "400 rows collected\n",
      "500 rows collected\n",
      "600 rows collected\n",
      "700 rows collected\n",
      "800 rows collected\n",
      "900 rows collected\n",
      "997 rows collected\n",
      "Data Collection Target Reached\n",
      "997 rows collected\n",
      "latest subreddit date: 2024-02-29 14:02:18\n"
     ]
    }
   ],
   "source": [
    "df = reddit_data(datetime(2023, 1, 20, 00, 00, 00, 342380))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import task\n",
    "import multiprocessing\n",
    "\n",
    "def multiprocessing_function(text_data):\n",
    "    \n",
    "    print(\"**Data Filtering in Progress**: This Process would take about 2-3 Minutes!\")\n",
    "    try:\n",
    "        with multiprocessing.Pool(processes=6) as pool:\n",
    "            res = pool.starmap(task, enumerate(text_data)) \n",
    "    except Exception as e:\n",
    "        print(\"exception in worker process\", e)\n",
    "        return text_data\n",
    "\n",
    "    res.sort(key=lambda x: x[0])\n",
    "    final_results = [result[1] for result in res]\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Data Filtering in Progress**: This Process would take about 2-3 Minutes!\n"
     ]
    }
   ],
   "source": [
    "df.text = multiprocessing_function(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I had a nightmare that I was in some sort of transparent pod and I had no skin on me at all, and no hands or feet, just muscle and bone truncated at the ankle and wrist, and I was still alive.  I guess that's how I feel as someone with a debilitating physical condition, kinda like a meat sack. Yeah, not one of my finer moments as a dreamer, for sure. \""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/veochae/Desktop/Dreams/data collection\n"
     ]
    }
   ],
   "source": [
    "print(Path.cwd())\n",
    "os.chdir('../app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.dropna()\n",
    "df.to_csv(\"./raw_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e871aec5cdce359f50730c2f4a4c8102d3246dd2d9815cdf4f3c7213e8de692"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
