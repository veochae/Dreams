{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import nltk\n",
    "from datetime import datetime, date\n",
    "import sys\n",
    "from better_profanity import profanity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from pathlib import Path\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df(df):\n",
    "   return df.to_csv(index=False).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduction():\n",
    "    st.title(\"Analyzing Dreams using NLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collection():\n",
    "    col1,col2,col3,col4 = st.columns()\n",
    "    with col1:\n",
    "        client_id = st.text_input(\"Reddit Client Id\")\n",
    "    with col2:\n",
    "        secret_key = st.text_input(\"Reddit Secret Key\")\n",
    "    with col3:\n",
    "        username = st.text_input(\"Reddit User Name\")\n",
    "    with col4:\n",
    "        password = st.text_input(\"Reddit Password\")\n",
    "    time_wanted = datetime(2023, 1, 20, 00, 00, 00, 342380)\n",
    "\n",
    "    client_id = client_id\n",
    "    secret_key = secret_key\n",
    "\n",
    "    auth = requests.auth.HTTPBasicAuth(client_id, secret_key)\n",
    "\n",
    "    data = {\n",
    "        'grant_type': 'password',\n",
    "        'username': username,\n",
    "        'password': password\n",
    "    }\n",
    "\n",
    "    headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token', \n",
    "                        auth = auth, \n",
    "                        data = data,\n",
    "                        headers = headers)\n",
    "\n",
    "    token = res.json()['access_token']\n",
    "\n",
    "    headers['Authorization'] = f'bearer {token}'    \n",
    "\n",
    "    def reddit_data(time_wanted):\n",
    "        stopper = 0\n",
    "\n",
    "        #initial set collection\n",
    "        res = requests.get('https://oauth.reddit.com/r/Dreams/new',\n",
    "                        headers = headers, params={'limit': '100'})\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for post in res.json()['data']['children']:\n",
    "            df = df.append({\n",
    "                'subreddit': post['data']['subreddit'],\n",
    "                'title': post['data']['title'],\n",
    "                'text': post['data']['selftext'],\n",
    "                'date': post['data']['created']\n",
    "            }, ignore_index = True)\n",
    "        \n",
    "        #further back collection\n",
    "        latest_key = post['kind'] + '_' + post['data']['id']\n",
    "\n",
    "        while df.tail(1)['date'][df.tail(1)['date'].index[0]] > datetime.timestamp(time_wanted):\n",
    "            for req in range(100):\n",
    "            \n",
    "                res = requests.get('https://oauth.reddit.com/r/Dreams/new',\n",
    "                                    headers = headers, \n",
    "                                    params={'limit': '100', 'after': latest_key})\n",
    "                \n",
    "                for post in res.json()['data']['children']:\n",
    "                    df = df.append({\n",
    "                        'subreddit': post['data']['subreddit'],\n",
    "                        'title': post['data']['title'],\n",
    "                        'text': post['data']['selftext'],\n",
    "                        'date': post['data']['created']\n",
    "                    }, ignore_index = True)\n",
    "\n",
    "                latest_key = post['kind'] + '_' + post['data']['id']\n",
    "                    \n",
    "                print(f'{len(df)} rows collected')\n",
    "\n",
    "                if len(df) >= 988:\n",
    "                    latest = df.tail(1)['date'][df.tail(1)['date'].index[0]]\n",
    "                    print(\"Data Collection Target Reached\")\n",
    "                    print(f'{len(df)} rows collected')\n",
    "                    print(f'latest subreddit date: {datetime.fromtimestamp(latest)}')\n",
    "                    return df\n",
    "\n",
    "                # time.sleep(5)\n",
    "        else: \n",
    "            print(\"Date Limit Reached\")\n",
    "            print(f'{len(df)} rows collected')\n",
    "            return df\n",
    "\n",
    "    df = reddit_data(time_wanted)\n",
    "\n",
    "    csv = convert_df(df)\n",
    "\n",
    "    st.download_button(\n",
    "    \"Press to Download\",\n",
    "    csv,\n",
    "    \"file.csv\",\n",
    "    \"text/csv\",\n",
    "    key='download-csv'\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('omw-1.4')\n",
    "    nltk.download('wordnet')\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "    df = pd.read_csv(st.file_uploader(label = \"Upload your Raw Data CSV\"), index_col= 0)\n",
    "\n",
    "    df= df.dropna()\n",
    "    df['date'] = [datetime.fromtimestamp(time) for time in df['date']]\n",
    "    df['text'] = [profanity.censor(i) for i in df['text']]\n",
    "\n",
    "    #calculating length of each dream\n",
    "    df['length'] = [len(j) for j in df['text']]\n",
    "\n",
    "    # if less than or equal to 5th percentile, assign t_f column False\n",
    "    df['t_f'] = [True if j > np.percentile(df['length'], 5) else False for j in df['length']]\n",
    "\n",
    "    #only keep t_f == True rows\n",
    "    semi = df.loc[df['t_f'] == True, :].__deepcopy__()\n",
    "\n",
    "    def clean(text):\n",
    "        text = re.sub('https?://\\S+|www\\.\\S+', '', text) #replace website urls\n",
    "        text = re.sub(r\"@\\S+\", '', text) #replace anything that follows @\n",
    "        text = re.sub(r\"#\\S+\", '', text) #replace anything that follows #\n",
    "        text = re.sub(r\"[0-9]\", '', text) #replace numeric\n",
    "        text = re.sub(r\"\\n\", '', text) #replace new line \n",
    "        text = re.sub(\"\\'m\", ' am ', text) \n",
    "        text = re.sub(\"\\'re\", ' are ', text) \n",
    "        text = re.sub(\"\\'d\", ' had ', text)\n",
    "        text = re.sub(\"\\'s\", ' is ', text)\n",
    "        text = re.sub(\"\\'ve\", ' have ', text)\n",
    "        text = re.sub(\" im \", ' i am ', text)\n",
    "        text = re.sub(\" iam \", ' i am ', text)\n",
    "        text = re.sub(\" youre \", ' you are ', text)\n",
    "        text = re.sub(\" theyre \", ' they are ', text)\n",
    "        text = re.sub(\" theyve \", ' they have ', text)\n",
    "        text = re.sub(\" weve \", ' we have ', text)\n",
    "        text = re.sub(\" isnt \", ' is not ', text)\n",
    "        text = re.sub(\" arent \", ' are not ', text)\n",
    "        text = re.sub(\" ur \", ' you are ', text)\n",
    "        text = re.sub(\" ive \", ' i have ', text)\n",
    "        text = re.sub(\"_\", '', text)\n",
    "        text = re.sub(\"\\\"\", '', text)\n",
    "        text = re.sub(\" bc \", ' because ', text)\n",
    "        text = re.sub(\" aka \", ' also known as ', text)\n",
    "        text = re.sub(\"√©\", 'e', text) #encoding error for é. replace it with e\n",
    "        text = re.sub(\" bf  \", ' boyfriend ', text)\n",
    "        text = re.sub(\" gf  \", ' girlfriend ', text)\n",
    "        text = re.sub(\" btw  \", ' by the way ', text)\n",
    "        text = re.sub(\" btwn  \", ' between ', text)\n",
    "        text = re.sub(r'([a-z])\\1{2,}', r'\\1', text) #if the same character is repeated more than twice, remove it to one. (E.A. ahhhhhh --> ah)\n",
    "        text = re.sub(' ctrl ', ' control ', text)\n",
    "        text = re.sub(' cuz ', ' because ', text)\n",
    "        text = re.sub(' dif ', ' different ', text)\n",
    "        text = re.sub(' dm ', ' direct message ', text)\n",
    "        text = re.sub(\"n't\", r' not ', text)\n",
    "        text = re.sub(\" fav \", ' favorite ', text)\n",
    "        text = re.sub(\" fave \", ' favorite ', text)\n",
    "        text = re.sub(\" fml \", \" fuck my life \", text)\n",
    "        text = re.sub(\" hq \", \" headquarter \", text)\n",
    "        text = re.sub(\" hr \", \" hours \", text)\n",
    "        text = re.sub(\" idk \",  \"i do not know \", text)\n",
    "        text = re.sub(\" ik \", ' i know ', text)\n",
    "        text = re.sub(\" lol \", ' laugh out loud ', text)\n",
    "        text = re.sub(\" u \", ' you ', text)\n",
    "        text = re.sub(\"√¶\", 'ae', text) #encoding error for áe. replace it with ae\n",
    "        text = re.sub(\"√® \", 'e', text) #encoding error for é. replace it with e\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def tokenization(text):\n",
    "        text = re.split('\\W+', text) #split words by whitespace to tokenize words\n",
    "        return text\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        text = [word for word in text if word not in stopword] #remove stopwords in the nltk stopwords dictionary\n",
    "        return text\n",
    "\n",
    "    def lemmatizer(text):\n",
    "        text = [wn.lemmatize(word) for word in text] #lemmatize the tokenized words. Lemmatized > Stemming in this case\n",
    "        return text                                  #because lemmatizing keeps the context of words alive\n",
    "\n",
    "    def vectorization(li):                            #create matrix of words and its respective presence for each dream\n",
    "        vectorizer = CountVectorizer()   \n",
    "        Xs = vectorizer.fit_transform(li)   \n",
    "        X = np.array(Xs.todense())\n",
    "        \n",
    "        return X\n",
    "\n",
    "    def get_column_name(li):                          #extract each word so that it will be present in corpus as column names\n",
    "        vectorizer = CountVectorizer()   \n",
    "        Xs = vectorizer.fit_transform(li)   \n",
    "        col_names=vectorizer.get_feature_names_out()\n",
    "        col_names = list(col_names)\n",
    "\n",
    "        return col_names\n",
    "\n",
    "    def extract_array(df):\n",
    "        clean_text = df['text'].apply(lambda x:clean(x.lower()))         #first clean the text on lower cased list of dreams\n",
    "        tokenized = clean_text.apply(lambda x: tokenization(x))          #tokenize the cleaned text\n",
    "        clean_text = tokenized.apply(lambda x: \" \".join(x))              #rejoin the words (just in case white space still present)\n",
    "        print(\"Complete: text cleaning\")\n",
    "        print(\"Complete: tokenization\")\n",
    "        x_stopwords = tokenized.apply(lambda x: remove_stopwords(x))     #remove stopwords from tokenized list\n",
    "        print(\"Complete: stopwords removed\")\n",
    "        lemmatized = x_stopwords.apply(lambda x: lemmatizer(x))          #lemmatize the removed stopwords word list\n",
    "        print(\"Complete: lemmatization\")\n",
    "        complete = lemmatized.apply(lambda x: \" \".join(x))               #rejoin the words so it will look like a sentence\n",
    "        mapx = vectorization(complete)                                   #start of mapping to corpus\n",
    "        name = get_column_name(complete)\n",
    "        mapx = pd.DataFrame(mapx, columns = name)\n",
    "        mapx.columns = name\n",
    "        print(\"Complete: vectorization\")\n",
    "        print(\"All Done!\")\n",
    "\n",
    "    clean_text, tokenized, x_stopwords, lemmatized, complete, corpus = extract_array(semi)\n",
    "\n",
    "    titles = ['clean_text', 'tokenized', 'x_stopwords', 'lemmatized', 'complete', 'corpus']\n",
    "\n",
    "    st.write(\"Download Clean Datasets\")\n",
    "    col1,col2,col3,col4,col5,col6,col7 = st.columns([1,1,1,1,1,1,1])\n",
    "\n",
    "    for k,context in enumerate(titles):\n",
    "        x = pd.DataFrame({'title': semi['title'],\n",
    "            context: vars()[context]}).reset_index()\n",
    "        x = x.drop(\"index\", axis =1)\n",
    "\n",
    "        vars()[f'{context}_csv'] = convert_df(x)\n",
    "\n",
    "        with vars()[f'col{k+1}']:\n",
    "            st.download_button(\n",
    "            f\"{context}\",\n",
    "            vars()[f'{context}_csv'],\n",
    "            f\"{context}.csv\",\n",
    "            \"text/csv\",\n",
    "            key='download-csv'\n",
    "\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_data\n",
    "def data_visualization_1():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    try:\n",
    "        uploaded = st.file_uploader(label = \"Upload your Complete Data CSV\")\n",
    "        df = pd.read_csv(uploaded, index_col= 0)\n",
    "\n",
    "        \n",
    "        tag_dict = {\"word\" :[], \"tag\":[]}\n",
    "        full = []\n",
    "\n",
    "        for i in df['complete']:\n",
    "            sent = nlp(i)\n",
    "            for j in sent:\n",
    "                tag_dict['word'].append(j.text)\n",
    "                tag_dict['tag'].append(j.tag_)\n",
    "\n",
    "        tag_df  = pd.DataFrame(tag_dict)\n",
    "\n",
    "        def barplot(x, z=\"\", l = False):\n",
    "            t = np.unique(x, return_counts = True)\n",
    "            s = np.argsort(t[1])\n",
    "\n",
    "            if l == True:\n",
    "                x = t[0][s][-z:]\n",
    "                y = t[1][s][-z:]\n",
    "            else:   \n",
    "                x = t[0][s]\n",
    "                y = t[1][s]\n",
    "\n",
    "            fig = plt.figure(figsize=(8,8)) #INITIALIZE FIGURE \n",
    "            ax = fig.add_subplot()\n",
    "\n",
    "            ax.bar(x,y)\n",
    "\n",
    "            ax.set_title(f\"Barplot\", fontsize = 20)\n",
    "            ax.set_xlabel(f\"\", fontsize = 15)\n",
    "            ax.set_ylabel(\"Count\", fontsize = 15)\n",
    "            plt.xticks(rotation=90)\n",
    "            st.pyplot(fig)    \n",
    "\n",
    "        barplot(tag_df['tag'])\n",
    "\n",
    "        def wordcloud(x, lim):\n",
    "            text = \" \".join(x)\n",
    "            cloud = WordCloud(collocations = False, max_words = lim).generate(text)\n",
    "            plt.imshow(cloud, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "            st.pyplot()\n",
    "\n",
    "        with st.container():\n",
    "            tab1, tab2, tab3 = st.tabs(np.unique(tag_df['tag'])[:3])\n",
    "\n",
    "            for i in np.unique(tag_df['tag']):\n",
    "                if i <3:\n",
    "                    with vars()[f'tab{i+1}']:\n",
    "                        temp = list(tag_df.loc[tag_df['tag'] == i , 'word'])\n",
    "                        print(i)\n",
    "                        #print(i, tag_desc[i])\n",
    "                        try:\n",
    "                            wordcloud(temp, lim = 100)\n",
    "                        except ValueError:\n",
    "                            print(\"Word Cloud ValueError \\n\")\n",
    "                            pass\n",
    "                else: break\n",
    "\n",
    "    except ValueError:\n",
    "        st.write(\"Please Upload the CSV file\")    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_of_speech_tag():\n",
    "    st.title(\"Part of Speech Tagging Visualization\")\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    try:\n",
    "        uploaded = st.file_uploader(label = \"Upload your Semi Raw Data CSV\")\n",
    "        df = pd.read_csv(uploaded, index_col= 0)   \n",
    "\n",
    "        row_n = int(st.text_input(\"Which Dream Would You Like to Examine? (In Row Number)\"))\n",
    "        sentence = list(nlp(df['text'][row_n]).sents) \n",
    "        rend = displacy.render(sentence, style = \"dep\", jupyter=False, options = {'compact': True})\n",
    "\n",
    "        st.image(rend, width=400, use_column_width='never')\n",
    "\n",
    "    except ValueError:\n",
    "        st.write(\"Please Upload the CSV file\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda():\n",
    "    #only caveat will be that ldavisualization cannot be embedded into the document, but we will need to prompt the users to go outside the page\n",
    "    print(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf():\n",
    "    print(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_names_to_funcs = {\n",
    "    \"Introduction\": introduction,\n",
    "    \"Data Collection\": data_collection,\n",
    "    \"Data Cleaning\": data_cleaning,\n",
    "    \"Data Visualization\": data_visualization_1,\n",
    "    \"Part of Speech Tagging\": part_of_speech_tag,\n",
    "    \"Name Identity Recognition\": name_identity_recognition,\n",
    "    \"Latency Discriminant Analysis (LDA)\": lda,\n",
    "    \"TF-IDF\": tf_idf,\n",
    "}\n",
    "\n",
    "demo_name = st.sidebar.selectbox(\"Please Select a Page\", page_names_to_funcs.keys())\n",
    "page_names_to_funcs[demo_name]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANLY501",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
