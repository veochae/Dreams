{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/veochae/Desktop/Dreams/data cleaning\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import nltk\n",
    "from datetime import datetime, date\n",
    "import sys\n",
    "\n",
    "#check for current working path, and set the working path to the data folder\n",
    "\n",
    "print(Path.cwd())\n",
    "os.chdir('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "df = pd.read_csv(\"./raw data/raw_data.csv\", index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the datetime from timestamp to datetime for ease of understanding\n",
    "df['date'] = [datetime.fromtimestamp(time) for time in df['date']]\n",
    "df= df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating length of each dream\n",
    "df['length'] = [len(j) for j in df['text']]\n",
    "\n",
    "# if less than or equal to 5th percentile, assign t_f column False\n",
    "df['t_f'] = [True if j > np.percentile(df['length'], 5) else False for j in df['length']]\n",
    "\n",
    "#only keep t_f == True rows\n",
    "semi = df.loc[df['t_f'] == True, :].__deepcopy__()\n",
    "\n",
    "#export semi-raw dataset\n",
    "semi.to_csv(\"./raw data/semi_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/veochae/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/veochae/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "     text = re.sub('https?://\\S+|www\\.\\S+', '', text) #replace website urls\n",
    "     text = re.sub(r\"@\\S+\", '', text) #replace anything that follows @\n",
    "     text = re.sub(r\"#\\S+\", '', text) #replace anything that follows #\n",
    "     text = re.sub(r\"[0-9]\", '', text) #replace numeric\n",
    "     text = re.sub(r\"\\n\", '', text) #replace new line \n",
    "     text = re.sub(\"\\'m\", ' am ', text) \n",
    "     text = re.sub(\"\\'re\", ' are ', text) \n",
    "     text = re.sub(\"\\'d\", ' had ', text)\n",
    "     text = re.sub(\"\\'s\", ' is ', text)\n",
    "     text = re.sub(\"\\'ve\", ' have ', text)\n",
    "     text = re.sub(\" im \", ' i am ', text)\n",
    "     text = re.sub(\" iam \", ' i am ', text)\n",
    "     text = re.sub(\" youre \", ' you are ', text)\n",
    "     text = re.sub(\" theyre \", ' they are ', text)\n",
    "     text = re.sub(\" theyve \", ' they have ', text)\n",
    "     text = re.sub(\" weve \", ' we have ', text)\n",
    "     text = re.sub(\" isnt \", ' is not ', text)\n",
    "     text = re.sub(\" arent \", ' are not ', text)\n",
    "     text = re.sub(\" ur \", ' you are ', text)\n",
    "     text = re.sub(\" ive \", ' i have ', text)\n",
    "     text = re.sub(\"_\", '', text)\n",
    "     text = re.sub(\"\\\"\", '', text)\n",
    "     text = re.sub(\" bc \", ' because ', text)\n",
    "     text = re.sub(\" aka \", ' also known as ', text)\n",
    "     text = re.sub(\"√©\", 'e', text) #encoding error for é. replace it with e\n",
    "     text = re.sub(\" bf  \", ' boyfriend ', text)\n",
    "     text = re.sub(\" gf  \", ' girlfriend ', text)\n",
    "     text = re.sub(\" btw  \", ' by the way ', text)\n",
    "     text = re.sub(\" btwn  \", ' between ', text)\n",
    "     text = re.sub(r'([a-z])\\1{2,}', r'\\1', text) #if the same character is repeated more than twice, remove it to one. (E.A. ahhhhhh --> ah)\n",
    "     text = re.sub(' ctrl ', ' control ', text)\n",
    "     text = re.sub(' cuz ', ' because ', text)\n",
    "     text = re.sub(' dif ', ' different ', text)\n",
    "     text = re.sub(' dm ', ' direct message ', text)\n",
    "     text = re.sub(\"n't\", r' not ', text)\n",
    "     text = re.sub(\" fav \", ' favorite ', text)\n",
    "     text = re.sub(\" fave \", ' favorite ', text)\n",
    "     # text = re.sub(\" fk \", \" fuck \", text)\n",
    "     # text = re.sub(\" fkin \", \" fucking \", text)\n",
    "     # text = re.sub(\" fkn \", \" fucking \", text)\n",
    "     text = re.sub(\" fml \", \" fuck my life \", text)\n",
    "     text = re.sub(\" hq \", \" headquarter \", text)\n",
    "     text = re.sub(\" hr \", \" hours \", text)\n",
    "     text = re.sub(\" idk \",  \"i do not know \", text)\n",
    "     text = re.sub(\" ik \", ' i know ', text)\n",
    "     text = re.sub(\" lol \", ' laugh out loud ', text)\n",
    "     text = re.sub(\" u \", ' you ', text)\n",
    "     text = re.sub(\"√¶\", 'ae', text) #encoding error for áe. replace it with ae\n",
    "     text = re.sub(\"√® \", 'e', text) #encoding error for é. replace it with e\n",
    "\n",
    "     # text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "     # text = re.sub('\\[.*?\\]', '', text)\n",
    "     # text = re.sub('\\n', '', text)\n",
    "     # text = re.sub('\\w*\\d\\w*', '', text)\n",
    "     # text = re.sub('<.*?>+', '', text)\n",
    "     # text = re.sub('(?<=:)\\w-', '', text)\n",
    "     # text = re.sub('(?<=@)\\w+', '', text)\n",
    "     # text = re.sub('@', '', text)\n",
    "     # text = re.sub(':', '', text)\n",
    "     # text = re.sub('_', \"\", text)\n",
    "     # text = re.sub(\"&amp;#;\", \"\", text)\n",
    "     text = text.strip()\n",
    "     return text\n",
    "\n",
    "def tokenization(text):\n",
    "     text = re.split('\\W+', text) #split words by whitespace to tokenize words\n",
    "     return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "     text = [word for word in text if word not in stopword] #remove stopwords in the nltk stopwords dictionary\n",
    "     return text\n",
    "\n",
    "def lemmatizer(text):\n",
    "     text = [wn.lemmatize(word) for word in text] #lemmatize the tokenized words. Lemmatized > Stemming in this case\n",
    "     return text                                  #because lemmatizing keeps the context of words alive\n",
    "\n",
    "def vectorization(li):                            #create matrix of words and its respective presence for each dream\n",
    "    vectorizer = CountVectorizer()   \n",
    "    Xs = vectorizer.fit_transform(li)   \n",
    "    X = np.array(Xs.todense())\n",
    "    \n",
    "    return X\n",
    "\n",
    "def get_column_name(li):                          #extract each word so that it will be present in corpus as column names\n",
    "     vectorizer = CountVectorizer()   \n",
    "     Xs = vectorizer.fit_transform(li)   \n",
    "     col_names=vectorizer.get_feature_names_out()\n",
    "     col_names = list(col_names)\n",
    "\n",
    "     return col_names\n",
    "\n",
    "def extract_array(df):\n",
    "     clean_text = df['text'].apply(lambda x:clean(x.lower()))         #first clean the text on lower cased list of dreams\n",
    "     tokenized = clean_text.apply(lambda x: tokenization(x))          #tokenize the cleaned text\n",
    "     clean_text = tokenized.apply(lambda x: \" \".join(x))              #rejoin the words (just in case white space still present)\n",
    "     print(\"Complete: text cleaning\")\n",
    "     print(\"Complete: tokenization\")\n",
    "     x_stopwords = tokenized.apply(lambda x: remove_stopwords(x))     #remove stopwords from tokenized list\n",
    "     print(\"Complete: stopwords removed\")\n",
    "     lemmatized = x_stopwords.apply(lambda x: lemmatizer(x))          #lemmatize the removed stopwords word list\n",
    "     print(\"Complete: lemmatization\")\n",
    "     complete = lemmatized.apply(lambda x: \" \".join(x))               #rejoin the words so it will look like a sentence\n",
    "     mapx = vectorization(complete)                                   #start of mapping to corpus\n",
    "     name = get_column_name(complete)\n",
    "     mapx = pd.DataFrame(mapx, columns = name)\n",
    "     mapx.columns = name\n",
    "     print(\"Complete: vectorization\")\n",
    "     print(\"All Done!\")\n",
    "\n",
    "     return clean_text, tokenized, x_stopwords, lemmatized, complete, mapx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: text cleaning\n",
      "Complete: tokenization\n",
      "Complete: stopwords removed\n",
      "Complete: lemmatization\n",
      "Complete: vectorization\n",
      "All Done!\n"
     ]
    }
   ],
   "source": [
    "#run the main function\n",
    "clean_text, tokenized, x_stopwords, lemmatized, complete, corpus = extract_array(semi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower title name as well just in case we want to compare with dream content later\n",
    "semi['title'] = [j.lower() for j in semi['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list to be used as index in forloop\n",
    "titles = ['clean_text', 'tokenized', 'x_stopwords', 'lemmatized', 'complete']\n",
    "\n",
    "#take the title and each cleaned versions of the text, transform to dataframe, then export to csv\n",
    "for context in titles:\n",
    "    x = pd.DataFrame({'title': semi['title'],\n",
    "          context: vars()[context]}).reset_index()\n",
    "    x = x.drop(\"index\", axis =1)\n",
    "    x.to_csv(f'./cleaned data/{context}.csv')\n",
    "\n",
    "#corpus is a dataframe of its own since it is a whole new matrix. Therefore, extracted to csv separately.\n",
    "corpus.to_csv(\"./cleaned data/corpus.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e871aec5cdce359f50730c2f4a4c8102d3246dd2d9815cdf4f3c7213e8de692"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
